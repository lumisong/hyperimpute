{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load reference datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_30260\\3696782721.py:29: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"airfoil\": pd.read_csv(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'xlrd'. Install xlrd >= 2.0.1 for xls Excel support Use pip or conda to install xlrd.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32me:\\Desktop\\插补实验\\hyperimpute\\hyperimpute\\lib\\site-packages\\pandas\\compat\\_optional.py:142\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Desktop\\插补实验\\hyperimpute\\hyperimpute\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:973\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xlrd'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m\n\u001b[0;32m     22\u001b[0m climate_model_samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m     skiprows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     26\u001b[0m climate_model_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(climate_model_samples)\n\u001b[0;32m     28\u001b[0m raw_datasets \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mairfoil\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/00291/airfoil_self_noise.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m         header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     32\u001b[0m         sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m     ),\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblood\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m     ),\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbc\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_raw_breast_cancer,\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalifornia\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_raw_california,\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclimate\u001b[39m\u001b[38;5;124m\"\u001b[39m: climate_model_df,\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslump\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/concrete/slump/slump_test.data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     45\u001b[0m     ),\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msonar\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     48\u001b[0m         header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m     ),\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiabetes\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_raw_diab,\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwine_red\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     53\u001b[0m         sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     54\u001b[0m     ),\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwine_white\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     57\u001b[0m         sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     58\u001b[0m     ),\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myeast\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     61\u001b[0m         sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     62\u001b[0m         header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     63\u001b[0m     ),\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miris\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_raw_iris,\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibras\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m         sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     68\u001b[0m         header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     69\u001b[0m     ),\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparkinsons\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     72\u001b[0m         sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     73\u001b[0m     ),\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myacht\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     76\u001b[0m         sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     77\u001b[0m         header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     78\u001b[0m     ),\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mionosphere\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m         sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     82\u001b[0m         header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     83\u001b[0m     ),\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mletter\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     86\u001b[0m         header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     87\u001b[0m     ),\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspam\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     ),\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcredit\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     93\u001b[0m         header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     94\u001b[0m     ),\n\u001b[0;32m     95\u001b[0m }\n",
      "File \u001b[1;32me:\\Desktop\\插补实验\\hyperimpute\\hyperimpute\\lib\\site-packages\\pandas\\io\\excel\\_base.py:478\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    477\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m     )\n",
      "File \u001b[1;32me:\\Desktop\\插补实验\\hyperimpute\\hyperimpute\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1513\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m engine\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options \u001b[38;5;241m=\u001b[39m storage_options\n\u001b[1;32m-> 1513\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Desktop\\插补实验\\hyperimpute\\hyperimpute\\lib\\site-packages\\pandas\\io\\excel\\_xlrd.py:34\u001b[0m, in \u001b[0;36mXlrdReader.__init__\u001b[1;34m(self, filepath_or_buffer, storage_options)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03mReader using xlrd engine.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m{storage_options}\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m err_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstall xlrd >= 2.0.1 for xls Excel support\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 34\u001b[0m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxlrd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(filepath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "File \u001b[1;32me:\\Desktop\\插补实验\\hyperimpute\\hyperimpute\\lib\\site-packages\\pandas\\compat\\_optional.py:145\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 145\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'xlrd'. Install xlrd >= 2.0.1 for xls Excel support Use pip or conda to install xlrd."
     ]
    }
   ],
   "source": [
    "#!pip install xlrd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "def smooth_line(src: list) -> list:\n",
    "    return signal.savgol_filter(src, 3, 1)\n",
    "\n",
    "\n",
    "X_raw_diab, _ = load_diabetes(as_frame=True, return_X_y=True)\n",
    "\n",
    "X_raw_breast_cancer, _ = load_breast_cancer(as_frame=True, return_X_y=True)\n",
    "X_raw_california, _ = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "X_raw_iris, y_raw_iris = load_iris(as_frame=True, return_X_y=True)\n",
    "\n",
    "climate_model_samples = np.loadtxt(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat\",\n",
    "    skiprows=1,\n",
    ")\n",
    "climate_model_df = pd.DataFrame(climate_model_samples)\n",
    "\n",
    "raw_datasets = {\n",
    "    \"airfoil\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/00291/airfoil_self_noise.dat\",\n",
    "        header=None,\n",
    "        sep=\"\\\\t\",\n",
    "    ),\n",
    "    \"blood\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\"\n",
    "    ),\n",
    "    \"bc\": X_raw_breast_cancer,\n",
    "    \"california\": X_raw_california,\n",
    "    \"climate\": climate_model_df,\n",
    "    \"compression\": pd.read_excel(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\"\n",
    "    ),\n",
    "    \"slump\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/slump/slump_test.data\"\n",
    "    ),\n",
    "    \"sonar\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\",\n",
    "        header=None,\n",
    "    ),\n",
    "    \"diabetes\": X_raw_diab,\n",
    "    \"wine_red\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
    "        sep=\";\",\n",
    "    ),\n",
    "    \"wine_white\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\",\n",
    "        sep=\";\",\n",
    "    ),\n",
    "    \"yeast\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data\",\n",
    "        sep=\"\\s+\",\n",
    "        header=None,\n",
    "    ),\n",
    "    \"iris\": X_raw_iris,\n",
    "    \"libras\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data\",\n",
    "        sep=\",\",\n",
    "        header=None,\n",
    "    ),\n",
    "    \"parkinsons\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\",\n",
    "        sep=\",\",\n",
    "    ),\n",
    "    \"yacht\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data\",\n",
    "        sep=\"\\s+\",\n",
    "        header=None,\n",
    "    ),\n",
    "    \"ionosphere\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\",\n",
    "        sep=\",\",\n",
    "        header=None,\n",
    "    ),\n",
    "    \"letter\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\",\n",
    "        header=None,\n",
    "    ),\n",
    "    \"spam\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "    ),\n",
    "    \"credit\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\",\n",
    "        header=None,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from hyperimpute.plugins.imputers import Imputers\n",
    "from hyperimpute.utils.distributions import enable_reproducible_results\n",
    "import hyperimpute.logger as log\n",
    "\n",
    "from hyperimpute.utils.benchmarks import compare_models\n",
    "\n",
    "enable_reproducible_results()\n",
    "\n",
    "imputers = Imputers()\n",
    "\n",
    "\n",
    "def get_imputer():\n",
    "    return imputers.get(\n",
    "        \"hyperimpute\",\n",
    "        optimizer=\"simple\",\n",
    "        classifier_seed=[\"random_forest\", \"logistic_regression\", \"xgboost\", \"catboost\"],\n",
    "        regression_seed=[\n",
    "            \"random_forest_regressor\",\n",
    "            \"linear_regression\",\n",
    "            \"xgboost_regressor\",\n",
    "            \"catboost_regressor\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_dataset_repeated(\n",
    "    name,\n",
    "    X_raw,\n",
    "    y,\n",
    "    ref_methods=[\"sklearn_ice\", \"sklearn_missforest\", \"sinkhorn\", \"miwae\", \"gain\"],\n",
    "    scenarios=[\"MAR\", \"MCAR\", \"MNAR\"],\n",
    "    miss_pct=[0.3],\n",
    "    n_iter=10,\n",
    "    debug=False,\n",
    "):\n",
    "    return compare_models(\n",
    "        name=name,\n",
    "        evaluated_model=get_imputer(),\n",
    "        X_raw=X_raw,\n",
    "        ref_methods=ref_methods,\n",
    "        scenarios=scenarios,\n",
    "        miss_pct=miss_pct,\n",
    "        n_iter=n_iter,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\",\n",
    "    header=None,\n",
    ")\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == \"object\":\n",
    "        df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "last_col = df.columns[-1]\n",
    "y = df[last_col]\n",
    "X_raw = df.drop(columns=[last_col])\n",
    "\n",
    "X_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results_by_df_size = {}\n",
    "for subsample in [1000, 4000, 7000, 10000, 13000, 17000, 20000]:\n",
    "    try:\n",
    "        with open(\"general_results/error_by_df_size.json\") as f:\n",
    "            results_by_df_size = json.load(f)\n",
    "    except BaseException:\n",
    "        pass\n",
    "\n",
    "    if str(subsample) in results_by_df_size:\n",
    "        print(\"subsample already cached\", subsample)\n",
    "        continue\n",
    "\n",
    "    print(\"subsample eval\", subsample)\n",
    "    X_local = X_raw.sample(subsample)\n",
    "    y_local = y[X_local.index]\n",
    "\n",
    "    results_by_df_size[subsample] = evaluate_dataset_repeated(\n",
    "        f\"subsample_{subsample}\", X_local, y_local\n",
    "    )\n",
    "\n",
    "    with open(\"general_results/error_by_df_size.json\", \"w\") as f:\n",
    "        json.dump(results_by_df_size, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By feature count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results_by_feat_count = {}\n",
    "\n",
    "for feat_count in [2, 4, 6, 8, 10, 12, 14]:\n",
    "    try:\n",
    "        with open(\"general_results/error_by_df_feat_count.json\") as f:\n",
    "            results_by_feat_count = json.load(f)\n",
    "    except BaseException:\n",
    "        pass\n",
    "\n",
    "    if str(feat_count) in results_by_feat_count:\n",
    "        print(\"feat count cached\", feat_count)\n",
    "        continue\n",
    "\n",
    "    print(\"eval feat count\", feat_count)\n",
    "\n",
    "    X_local = X_raw.sample(2000)\n",
    "    y_local = y[X_local.index]\n",
    "    X_local = X_local.iloc[:, :feat_count]\n",
    "\n",
    "    results_by_feat_count[feat_count] = evaluate_dataset_repeated(\n",
    "        f\"feature_subsample_{feat_count}\", X_local, y_local\n",
    "    )\n",
    "\n",
    "    with open(\"general_results/error_by_df_feat_count.json\", \"w\") as f:\n",
    "        json.dump(results_by_feat_count, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xlrd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "\n",
    "def smooth_line(src: list) -> list:\n",
    "    return signal.savgol_filter(src, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_models = {\n",
    "    \"Evaluated: hyperimpute\": \"hyperimpute\",\n",
    "    \"Our method\": \"hyperimpute\",\n",
    "    \"sklearn_missforest\": \"missforest\",\n",
    "    \"sklearn_ice\": \"ice\",\n",
    "}\n",
    "\n",
    "\n",
    "def process_testcase_data(results):\n",
    "    error_data = {}\n",
    "\n",
    "    for size in results:\n",
    "        for scenario_idx in range(len(results[size][\"rmse\"])):\n",
    "            headers = results[size][\"headers\"][2:]\n",
    "\n",
    "            for idx, h in enumerate(headers):\n",
    "                if h in remap_models:\n",
    "                    headers[idx] = remap_models[h]\n",
    "\n",
    "            rmse = results[size][\"rmse\"][scenario_idx][2:]\n",
    "            wasserstein = results[size][\"wasserstein\"][scenario_idx][2:]\n",
    "            scenario = results[size][\"rmse\"][scenario_idx][0]\n",
    "\n",
    "            if scenario not in error_data:\n",
    "                error_data[scenario] = {\"rmse\": {}, \"wasserstein\": {}}\n",
    "\n",
    "            for idx, header in enumerate(headers):\n",
    "                if header not in error_data[scenario][\"rmse\"]:\n",
    "                    error_data[scenario][\"rmse\"][header] = {\"mean\": [], \"std\": []}\n",
    "                    error_data[scenario][\"wasserstein\"][header] = {\n",
    "                        \"mean\": [],\n",
    "                        \"std\": [],\n",
    "                    }\n",
    "\n",
    "                error_data[scenario][\"rmse\"][header][\"mean\"].append(rmse[idx][0])\n",
    "                error_data[scenario][\"rmse\"][header][\"std\"].append(rmse[idx][1])\n",
    "                error_data[scenario][\"wasserstein\"][header][\"mean\"].append(\n",
    "                    wasserstein[idx][0]\n",
    "                )\n",
    "                error_data[scenario][\"wasserstein\"][header][\"std\"].append(\n",
    "                    wasserstein[idx][1]\n",
    "                )\n",
    "    return error_data\n",
    "\n",
    "\n",
    "# by feature count\n",
    "with open(\"general_results/error_by_df_feat_count.json\") as f:\n",
    "    results_by_feat_count = json.load(f)\n",
    "# by df size\n",
    "with open(\"general_results/error_by_df_size.json\") as f:\n",
    "    results_by_df_size = json.load(f)\n",
    "\n",
    "# by miss ratio\n",
    "\n",
    "\n",
    "def generate_mean_std(data, headers):\n",
    "    _mean = []\n",
    "    _std = []\n",
    "\n",
    "    for scenario in data:\n",
    "        local_mean = []\n",
    "        local_std = []\n",
    "        for vals in scenario:\n",
    "            if isinstance(vals, list):\n",
    "                local_mean.append(vals[0])\n",
    "                local_std.append(vals[1])\n",
    "            else:\n",
    "                local_mean.append(vals)\n",
    "                local_std.append(vals)\n",
    "        _mean.append(local_mean)\n",
    "        _std.append(local_std)\n",
    "    _mean_df = pd.DataFrame(_mean, columns=headers)\n",
    "    _std_df = pd.DataFrame(_std, columns=headers)\n",
    "\n",
    "    return _mean_df, _std_df\n",
    "\n",
    "\n",
    "f_miss_ratio = Path(\"general_results/error_by_miss_ratio.json\")\n",
    "\n",
    "if not f_miss_ratio.exists():\n",
    "    for src in [\n",
    "        \"experiments_01_hyperimpute_with_hyperband\",\n",
    "        \"experiments_01_hyperimpute_with_naive_search\",\n",
    "    ]:\n",
    "        try:\n",
    "            shutil.copy(Path(src) / \"letter\", f_miss_ratio)\n",
    "        except BaseException as e:\n",
    "            print(e)\n",
    "\n",
    "with open(\"general_results/error_by_miss_ratio.json\") as f:\n",
    "    raw_results_by_miss_ratio = json.load(f)\n",
    "\n",
    "    headers = raw_results_by_miss_ratio[\"headers\"]\n",
    "\n",
    "    rmse_mean, rmse_std = generate_mean_std(raw_results_by_miss_ratio[\"rmse\"], headers)\n",
    "    distr_mean, distr_std = generate_mean_std(\n",
    "        raw_results_by_miss_ratio[\"wasserstein\"], headers\n",
    "    )\n",
    "\n",
    "    raw_data = {\n",
    "        \"rmse\": (rmse_mean, rmse_std),\n",
    "        \"wasserstein\": (distr_mean, distr_std),\n",
    "    }\n",
    "\n",
    "    results_by_miss_ratio = {}\n",
    "\n",
    "    for metric in raw_data:\n",
    "        df, df_std = raw_data[metric]\n",
    "\n",
    "        # Prediction norm\n",
    "        num_df = df._get_numeric_data()\n",
    "        num_df[num_df <= 0] = 1e-6\n",
    "\n",
    "        for scenario in [\"MAR\", \"MCAR\", \"MNAR\"]:\n",
    "            if scenario not in results_by_miss_ratio:\n",
    "                results_by_miss_ratio[scenario] = {}\n",
    "\n",
    "            for miss in [0.1, 0.3, 0.5, 0.7]:\n",
    "                if miss not in results_by_miss_ratio[scenario]:\n",
    "                    results_by_miss_ratio[scenario][miss] = {}\n",
    "\n",
    "                local_df = df[df[\"Scenario\"] == scenario].drop(columns=[\"Scenario\"])\n",
    "                local_df = local_df[local_df[\"miss_pct [0, 1]\"] == miss].drop(\n",
    "                    columns=[\"miss_pct [0, 1]\"]\n",
    "                )\n",
    "\n",
    "                local_df = local_df.rename(columns=remap_models)\n",
    "\n",
    "                if len(local_df) == 0:\n",
    "                    continue\n",
    "\n",
    "                local_df_std = df_std[df_std[\"Scenario\"] == scenario].drop(\n",
    "                    columns=[\"Scenario\"]\n",
    "                )\n",
    "                local_df_std = local_df_std[\n",
    "                    local_df_std[\"miss_pct [0, 1]\"] == miss\n",
    "                ].drop(columns=[\"miss_pct [0, 1]\"])\n",
    "\n",
    "                local_df_std = local_df_std.rename(columns=remap_models)\n",
    "\n",
    "                if metric not in results_by_miss_ratio[scenario][miss]:\n",
    "                    results_by_miss_ratio[scenario][miss][metric] = {}\n",
    "                for col in local_df.columns:\n",
    "                    if col not in results_by_miss_ratio[scenario][miss][metric]:\n",
    "                        results_by_miss_ratio[scenario][miss][metric][col] = {\n",
    "                            \"mean\": [],\n",
    "                            \"std\": [],\n",
    "                        }\n",
    "                    results_by_miss_ratio[scenario][miss][metric][col][\"mean\"].append(\n",
    "                        min(local_df[col].values[0], 0.5)\n",
    "                    )\n",
    "                    results_by_miss_ratio[scenario][miss][metric][col][\"std\"].append(\n",
    "                        min(local_df_std[col].values[0], 0.01)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot error by df size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_axis_df_size = list(results_by_df_size.keys())\n",
    "\n",
    "fontsize = 14\n",
    "\n",
    "output_dir = Path(\"diagrams\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "map_keys = {\"rmse\": \"RMSE\", \"wasserstein\": \"WD\"}\n",
    "\n",
    "error_by_df_size = process_testcase_data(results_by_df_size)\n",
    "\n",
    "\n",
    "def generate_plot_for_ax_df_size(ax, scenario, metric):\n",
    "    for model in error_by_df_size[scenario][metric]:\n",
    "        datapoints = error_by_df_size[scenario][metric][model][\"mean\"]\n",
    "        datapoints_std = error_by_df_size[scenario][metric][model][\"std\"]\n",
    "\n",
    "        ax.errorbar(\n",
    "            x_axis_df_size,\n",
    "            smooth_line(np.asarray(datapoints)),\n",
    "            yerr=np.asarray(datapoints_std),\n",
    "            label=str(model),\n",
    "            linewidth=2,\n",
    "        )\n",
    "\n",
    "    ax.set_xticks(x_axis_df_size, fontsize=fontsize)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=fontsize)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def generate_plot_df_size(scenario):\n",
    "    plt.style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "    metrics = list([\"rmse\", \"wasserstein\"])\n",
    "    fig, axs = plt.subplots(len(metrics), figsize=(10, 11))\n",
    "\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        generate_plot_for_ax_df_size(axs[idx], scenario, metric)\n",
    "\n",
    "        axs[idx].set_ylabel(map_keys[metric], fontsize=fontsize)\n",
    "\n",
    "    # plt.title(f\"{scenario} simulation with {miss} missingness\", fontdict = {\"fontsize\": 150}, loc = \"top\")\n",
    "\n",
    "    axs[0].legend(\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.15, 1.22),\n",
    "        ncol=int(len(error_by_df_size[scenario][\"rmse\"]) / 2),\n",
    "        prop={\"size\": fontsize},\n",
    "    )\n",
    "    # fig.suptitle(f\"{scenario} simulation: error by dataset size\", fontsize=fontsize)\n",
    "    plt.savefig(output_dir / f\"error_by_df_size_{scenario}.png\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for scenario in [\"MAR\", \"MCAR\", \"MNAR\"]:\n",
    "    print(\"scenario\", scenario)\n",
    "    generate_plot_df_size(scenario=scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By feature count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_axis_feat_count = list(results_by_feat_count.keys())\n",
    "\n",
    "fontsize = 14\n",
    "\n",
    "error_by_feat_count = process_testcase_data(results_by_feat_count)\n",
    "\n",
    "\n",
    "def generate_plot_for_ax_feat_count(ax, scenario, metric):\n",
    "    ## We selected the same dataset from every model, and average across missingness pct\n",
    "    for model in error_by_feat_count[scenario][metric]:\n",
    "        datapoints = error_by_feat_count[scenario][metric][model][\"mean\"]\n",
    "        datapoints_std = error_by_feat_count[scenario][metric][model][\"std\"]\n",
    "\n",
    "        ax.errorbar(\n",
    "            x_axis_feat_count,\n",
    "            smooth_line(np.asarray(datapoints)),\n",
    "            yerr=np.asarray(datapoints_std),\n",
    "            label=str(model),\n",
    "            linewidth=2,\n",
    "        )\n",
    "\n",
    "    ax.set_xticks(x_axis_feat_count, fontsize=fontsize)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=fontsize)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def generate_plot_feat_count(scenario):\n",
    "    plt.style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "    metrics = [\"rmse\", \"wasserstein\"]\n",
    "    fig, axs = plt.subplots(len(metrics), figsize=(10, 11))\n",
    "\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        generate_plot_for_ax_feat_count(axs[idx], scenario, metric)\n",
    "        axs[idx].set_ylabel(map_keys[metric], fontsize=fontsize)\n",
    "\n",
    "    axs[0].legend(\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.15, 1.22),\n",
    "        ncol=int(len(error_by_feat_count[scenario][\"rmse\"]) / 2),\n",
    "        prop={\"size\": fontsize},\n",
    "    )\n",
    "    plt.savefig(output_dir / f\"error_by_feature_cnt_{scenario}.png\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for scenario in [\"MAR\", \"MCAR\", \"MNAR\"]:\n",
    "    print(\"scenario \", scenario)\n",
    "    generate_plot_feat_count(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot by miss ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_axis_miss_ratio = [0.1, 0.3, 0.5, 0.7]\n",
    "\n",
    "fontsize = 14\n",
    "\n",
    "error_by_miss_ratio = results_by_miss_ratio\n",
    "\n",
    "\n",
    "def generate_plot_for_ax_miss_ratio(ax, scenario, metric):\n",
    "    barWidth = 1\n",
    "\n",
    "    max_val = 0\n",
    "    idx = 0\n",
    "\n",
    "    for model in error_by_miss_ratio[scenario][0.1][metric]:\n",
    "\n",
    "        datapoints = []\n",
    "        datapoints_std = []\n",
    "\n",
    "        for miss in error_by_miss_ratio[scenario]:\n",
    "            if metric not in error_by_miss_ratio[scenario][miss]:\n",
    "                continue\n",
    "\n",
    "            local_res = error_by_miss_ratio[scenario][miss][metric][model][\"mean\"][0]\n",
    "            local_res_std = error_by_miss_ratio[scenario][miss][metric][model][\"std\"][0]\n",
    "            datapoints.append(local_res)\n",
    "            datapoints_std.append(local_res_std)\n",
    "\n",
    "        ax.errorbar(\n",
    "            x_axis_miss_ratio,\n",
    "            smooth_line(datapoints),\n",
    "            yerr=datapoints_std,\n",
    "            label=str(model),\n",
    "            linewidth=2,\n",
    "            marker=\"o\",\n",
    "        )\n",
    "\n",
    "    ax.set_xticks(x_axis_miss_ratio, fontsize=fontsize)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=fontsize)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def generate_plot_miss_ratio(scenario):\n",
    "    plt.style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "    metrics = list(error_by_miss_ratio[scenario][0.1].keys())\n",
    "    if len(metrics) == 0:\n",
    "        return\n",
    "\n",
    "    fig, axs = plt.subplots(len(metrics), figsize=(10, 11))\n",
    "\n",
    "    models_cnt = len(error_by_miss_ratio[scenario][0.1][\"rmse\"].keys())\n",
    "\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        generate_plot_for_ax_miss_ratio(axs[idx], scenario, metric)\n",
    "        axs[idx].set_ylabel(map_keys[metric], fontsize=fontsize)\n",
    "\n",
    "    axs[0].legend(\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.1, 1.27),\n",
    "        ncol=int(models_cnt / 2),\n",
    "        prop={\"size\": fontsize},\n",
    "    )\n",
    "    # fig.suptitle(f\"{scenario} simulation\", fontsize=fontsize)\n",
    "    plt.savefig(output_dir / f\"error_by_miss_ratio_{scenario}.png\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for scenario in [\"MAR\", \"MCAR\", \"MNAR\"]:\n",
    "    generate_plot_miss_ratio(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot group metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_grouped_plot(scenario):\n",
    "    global results_by_miss_ratio\n",
    "\n",
    "    plt.style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "    metrics = list(results_by_miss_ratio[scenario][0.1].keys())\n",
    "    if len(metrics) == 0:\n",
    "        return\n",
    "\n",
    "    fig, axs = plt.subplots(len(metrics), 3, figsize=(20, 11))\n",
    "\n",
    "    mods_miss_ratio = list(results_by_miss_ratio[scenario][0.1][\"rmse\"].keys())\n",
    "    mods_feat_count = list(error_by_feat_count[scenario][\"rmse\"].keys())\n",
    "    mods_df_size = list(error_by_df_size[scenario][\"rmse\"].keys())\n",
    "\n",
    "    assert (\n",
    "        mods_feat_count == mods_df_size\n",
    "    ), f\"Invalid models. {mods_feat_count} vs {mods_df_size}\"\n",
    "    assert (\n",
    "        mods_feat_count == mods_miss_ratio\n",
    "    ), f\"Invalid models. {mods_feat_count} vs {mods_miss_ratio}\"\n",
    "\n",
    "    models_cnt = len(error_by_feat_count[scenario][\"rmse\"].keys())\n",
    "\n",
    "    for row_idx, metric in enumerate(metrics):\n",
    "        for col_idx, testcase_cbk in enumerate(\n",
    "            [\n",
    "                generate_plot_for_ax_df_size,\n",
    "                generate_plot_for_ax_feat_count,\n",
    "                generate_plot_for_ax_miss_ratio,\n",
    "            ]\n",
    "        ):\n",
    "            testcase_cbk(axs[row_idx][col_idx], scenario, metric)\n",
    "\n",
    "    axs[0][0].legend(\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.5, 1.15),\n",
    "        ncol=models_cnt,\n",
    "        prop={\"size\": fontsize},\n",
    "    )\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        axs[idx][0].set_ylabel(map_keys[metric], fontsize=fontsize)\n",
    "\n",
    "    for idx, testcase in enumerate(\n",
    "        [\"Observed data size\", \"Feature count\", \"Missigness rate\"]\n",
    "    ):\n",
    "        axs[1][idx].set_xlabel(testcase, fontsize=fontsize)\n",
    "\n",
    "    plt.savefig(output_dir / f\"error_by_all_grouped_{scenario}.png\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "## normalize\n",
    "new_results_by_miss_ratio = {}\n",
    "\n",
    "for scenario in [\"MAR\", \"MCAR\", \"MNAR\"]:\n",
    "    new_results_by_miss_ratio[scenario] = {}\n",
    "\n",
    "    for miss in results_by_miss_ratio[scenario]:\n",
    "        new_results_by_miss_ratio[scenario][miss] = {}\n",
    "        for metric in results_by_miss_ratio[scenario][miss]:\n",
    "            new_results_by_miss_ratio[scenario][miss][metric] = {}\n",
    "\n",
    "            mods_feat_count = list(error_by_feat_count[scenario][\"rmse\"].keys())\n",
    "            for mod in mods_feat_count:\n",
    "                new_results_by_miss_ratio[scenario][miss][metric][\n",
    "                    mod\n",
    "                ] = results_by_miss_ratio[scenario][miss][metric][mod]\n",
    "## normalize\n",
    "\n",
    "error_by_miss_ratio = new_results_by_miss_ratio\n",
    "\n",
    "for scenario in [\"MAR\", \"MCAR\", \"MNAR\"]:\n",
    "    print(\"scenario\", scenario)\n",
    "    generate_grouped_plot(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
